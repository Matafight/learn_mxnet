{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6637\n"
     ]
    }
   ],
   "source": [
    "#读取文件\n",
    "#_*_coding:utf-8_*_\n",
    "with open('./input/xiaoshuo.txt','r') as fh:\n",
    "    lines = fh.readlines()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "本书由 nainia520 整理\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(lines[0]))\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将list中所有元素拼接成一个长度字符串\n",
    "newlines = [item.replace('\\n',' ') for item in lines]\n",
    "content = ' '.join(newlines) \n",
    "content = ' '.join(content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193605\n"
     ]
    }
   ],
   "source": [
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct data iterator \n",
    "content = content[0:2000]\n",
    "idx_to_char = list(set(content))\n",
    "char_to_idx = dict([(char,int(i)) for i,char in enumerate(idx_to_char)])\n",
    "vocab_size = len(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_corpus = [char_to_idx[c] for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[94, 468, 410, 358, 469, 216, 273, 469, 273, 216]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import update\n",
    "\n",
    "def data_iter_random(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #随机生成 data iterator，用yield返回？？\n",
    "    #先随机分段\n",
    "    #再随机选择batch_size个字段\n",
    "    len_corpus=len(corpus)\n",
    "    num_patch = len_corpus//num_steps\n",
    "    num_batch = num_patch//batch_size \n",
    "    idx_list = [x for x in range(num_patch)]\n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    for batch in range(num_batch):\n",
    "        #在idx_list中随机选择batch_size 个元素，选完之后在idx_list中删除这些元素\n",
    "        to_use = random.sample(idx_list,batch_size)\n",
    "        batch_data = []\n",
    "        batch_label = []\n",
    "        for item in to_use:\n",
    "            idx_list.remove(item)\n",
    "        for step in to_use:\n",
    "            #i * num_patch ~ (i+1)*num_path\n",
    "            data = corpus[step*num_steps:(step+1)*num_steps]\n",
    "            batch_data.append(data)\n",
    "            #判断最后一个是否越界\n",
    "            if (step+1)*num_steps < len_corpus:\n",
    "                label = corpus[step*num_steps + 1:(step+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[step*num_steps+1:(step+1)*num_steps]\n",
    "                label.append('0')#填充字符\n",
    "            batch_label.append(label)\n",
    "        #聚合\n",
    "        #ndarray batch_size * num_steps\n",
    "        batch_data = mx.ndarray.array(batch_data)\n",
    "        batch_label = mx.ndarray.array(batch_label)\n",
    "        yield(batch_data,batch_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_iter = data_iter_random(3,10,idx_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count = 1\n",
    "#for item in data_iter:\n",
    "#    if count>10:\n",
    "#        break\n",
    "#    print(item[0])\n",
    "#    print(item[1])\n",
    "#    print(type(item[1]))\n",
    "#    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_iter_consective(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #连续采样\n",
    "    num_patch = len(corpus)//num_steps\n",
    "    num_batch = num_patch//batch_size\n",
    "    \n",
    "    \n",
    "    for i_batch in range(num_batch):\n",
    "        total_data = []\n",
    "        total_label = []\n",
    "        for i_batch_size in range(batch_size):\n",
    "            start_pos = i_batch+num_batch*(i_batch_size)\n",
    "            data = corpus[start_pos*num_steps:(start_pos+1)*num_steps]\n",
    "            #判断最后是否越界\n",
    "            if(start_pos+1)*num_steps < len(corpus):\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps]\n",
    "                label.append('0')\n",
    "            total_data.append(data)\n",
    "            total_label.append(label)\n",
    "        total_data = mx.ndarray.array(total_data)\n",
    "        total_label = mx.ndarray.array(total_label)\n",
    "        yield(total_data,total_label)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#验证 iter是否正确\n",
    "def valid_data_iter():\n",
    "    corpus = list(range(10000))\n",
    "    batch_size = 10\n",
    "    num_steps = 3\n",
    "    data_iter_rand = data_iter_consective(batch_size=batch_size,num_steps=num_steps,corpus=corpus)\n",
    "    count = 0\n",
    "    for item in data_iter_rand:\n",
    "        if(count>10):\n",
    "            break;\n",
    "        count +=1\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据还需要编码，采用最简单的one-hot-encoding 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "for data,label in data_iter_consective(3,10,idx_corpus):\n",
    "    inp =[nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    break\n",
    "    \n",
    "def one_hot(data,vocab_size):\n",
    "    inp = [nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    #len(inp) = num_steps\n",
    "    return inp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construct rnn\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "input_dim = vocab_size\n",
    "batch_size  = 10\n",
    "std = .01\n",
    "def get_params():\n",
    "    #set params\n",
    "    W_xh= nd.random_normal(scale=std,shape=(input_dim,hidden_dim))\n",
    "    W_hh = nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim))\n",
    "    b_h = nd.zeros(hidden_dim)\n",
    "    \n",
    "    W_hy = nd.random_normal(scale = std,shape=(hidden_dim,output_dim))\n",
    "    b_y = nd.zeros(output_dim)\n",
    "    params = [W_xh,W_hh,b_h,W_hy,b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn(inputs,states,*params):\n",
    "    #inputs  is a list shape = batch_size * input_dim\n",
    "    H = states\n",
    "    W_xh,W_hh,b_h,W_hy,b_y = params\n",
    "    outputs = [] \n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)\n",
    "        Y = nd.tanh(nd.dot(H,W_hy)+b_y)\n",
    "        outputs.append(Y)\n",
    "    return (outputs,H)\n",
    "        \n",
    "#test\n",
    "params = get_params()\n",
    "states = nd.zeros((batch_size,hidden_dim))\n",
    "inputs = [nd.ones((batch_size,input_dim))] *3\n",
    "(outputs,H) = rnn(inputs,states,*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training and inference\n",
    "\n",
    "def predict_rnn(rnn,prefix,vocab_size,num_char,params,hidden_dim,char_to_idx,idx_to_char,ctx=mx.cpu()):\n",
    "    H = nd.zeros(shape=(1,hidden_dim),ctx=ctx)\n",
    "    outputs = []\n",
    "    print(char_to_idx[prefix[0]])\n",
    "    outputs.append(char_to_idx[prefix[0]])\n",
    "    \n",
    "    for i in range(num_char+len(prefix)):\n",
    "        x = nd.array([outputs[-1]])\n",
    "        x = one_hot(x,vocab_size)\n",
    "        out, H = rnn(x,H,*params)\n",
    "        if i< len(prefix)-1:\n",
    "            new_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            new_input  = int(out[-1].argmax(axis=1).asscalar())\n",
    "        outputs.append(new_input)\n",
    "    print(outputs)\n",
    "    return ''.join([idx_to_char[x] for x in outputs])\n",
    "    \n",
    "#prefix='书由 nainia'\n",
    "#predict_rnn(rnn,prefix,vocab_size,10,params,hidden_dim,char_to_idx,idx_to_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient cliping\n",
    "def grad_clipping(params,theta,ctx):\n",
    "    #python传参是传的引用，所有不用返回值\n",
    "    if theta is not None:\n",
    "        norm = nd.array([0.0],ctx=ctx)\n",
    "        for p in params:\n",
    "            norm += nd.sum(p.grad**2)\n",
    "        norm = nd.sqrt(norm).asscalar()\n",
    "        if norm > theta:\n",
    "            for p in params:\n",
    "                p.grad[:] *= theta/norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "#perplexity 困惑度\n",
    "\n",
    "def train_and_predict(num_steps,ctx=mx.cpu()):\n",
    "    #how?\n",
    "    #定义损失\n",
    "    loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    #生成数据迭代器\n",
    "    #循环num_epoch\n",
    "    num_epochs = 10\n",
    "    \n",
    "    params = get_params()\n",
    "    for epoch in range(num_epochs):\n",
    "        data_iter = data_iter_random(num_steps,batch_size,idx_corpus)\n",
    "        train_loss = nd.array([0.0])\n",
    "        H = nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx)\n",
    "        for data,label in data_iter:\n",
    "            data = one_hot(nd.array(data),vocab_size)\n",
    "            label = nd.array(label)\n",
    "            with mx.autograd.record():\n",
    "                out,H = rnn(data,H,*params)\n",
    "                #计算loss\n",
    "                for i in range(len(out)):\n",
    "                    train_loss += nd.mean(loss(out[i],label[:,i])).asscalar()\n",
    "                    myloss = loss(out[i],label[:,i])\n",
    "                myloss.backward()\n",
    "                update(params,0.01)\n",
    "        print(train_loss)\n",
    "#还有要update，backward只是计算了梯度。\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 1270.6932373]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1262.22106934]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1254.78771973]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1247.7878418]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1235.91113281]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1215.15734863]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1208.78466797]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1206.9420166]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1204.1184082]\n",
      "<NDArray 1 @cpu(0)>\n",
      "\n",
      "[ 1203.54089355]\n",
      "<NDArray 1 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "num_steps=3\n",
    "train_and_predict(num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
