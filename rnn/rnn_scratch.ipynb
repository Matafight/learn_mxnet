{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6637\n"
     ]
    }
   ],
   "source": [
    "#读取文件\n",
    "#_*_coding:utf-8_*_\n",
    "with open('./input/xiaoshuo.txt','r') as fh:\n",
    "    lines = fh.readlines()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "本书由 nainia520 整理\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(lines[0]))\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将list中所有元素拼接成一个长度字符串\n",
    "newlines = [item.replace('\\n',' ') for item in lines]\n",
    "content = ' '.join(newlines) \n",
    "content = ' '.join(content.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193605\n"
     ]
    }
   ],
   "source": [
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct data iterator \n",
    "idx_to_char = list(set(content))\n",
    "char_to_idx = dict([(char,int(i)) for i,char in enumerate(idx_to_char)])\n",
    "vocab_size = len(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_corpus = [char_to_idx[c] for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[505, 2093, 1468, 782, 2719, 517, 1793, 2719, 1793, 517]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import mxnet as mx\n",
    "def data_iter_random(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #随机生成 data iterator，用yield返回？？\n",
    "    #先随机分段\n",
    "    #再随机选择batch_size个字段\n",
    "    len_corpus=len(corpus)\n",
    "    num_patch = len_corpus//num_steps\n",
    "    num_batch = num_patch//batch_size \n",
    "    idx_list = [x for x in range(num_patch)]\n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    for batch in range(num_batch):\n",
    "        #在idx_list中随机选择batch_size 个元素，选完之后在idx_list中删除这些元素\n",
    "        to_use = random.sample(idx_list,batch_size)\n",
    "        batch_data = []\n",
    "        batch_label = []\n",
    "        for item in to_use:\n",
    "            idx_list.remove(item)\n",
    "        for step in to_use:\n",
    "            #i * num_patch ~ (i+1)*num_path\n",
    "            data = corpus[step*num_steps:(step+1)*num_steps]\n",
    "            batch_data.append(data)\n",
    "            #判断最后一个是否越界\n",
    "            if (step+1)*num_steps < len_corpus:\n",
    "                label = corpus[step*num_steps + 1:(step+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[step*num_steps+1:(step+1)*num_steps]\n",
    "                label.append('0')#填充字符\n",
    "            batch_label.append(label)\n",
    "        #聚合\n",
    "        #ndarray batch_size * num_steps\n",
    "        batch_data = mx.ndarray.array(batch_data)\n",
    "        batch_label = mx.ndarray.array(batch_label)\n",
    "        yield(batch_data,batch_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_iter = data_iter_random(3,10,idx_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[  916.  1137.  1660.]\n",
      " [  999.  2238.   828.]\n",
      " [    5.  1098.   575.]\n",
      " [ 2850.  2432.  1280.]\n",
      " [  986.   318.  2586.]\n",
      " [  593.  1934.   438.]\n",
      " [  503.  1159.     5.]\n",
      " [  438.  1009.    33.]\n",
      " [ 2043.   402.   214.]\n",
      " [  782.  1065.    42.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[ 1137.  1660.  2089.]\n",
      " [ 2238.   828.  1763.]\n",
      " [ 1098.   575.   938.]\n",
      " [ 2432.  1280.   313.]\n",
      " [  318.  2586.   593.]\n",
      " [ 1934.   438.   325.]\n",
      " [ 1159.     5.  2127.]\n",
      " [ 1009.    33.  2147.]\n",
      " [  402.   214.  1960.]\n",
      " [ 1065.    42.   426.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[  1.98500000e+03   1.88600000e+03   1.15900000e+03]\n",
      " [  1.84700000e+03   2.12400000e+03   1.94600000e+03]\n",
      " [  2.43200000e+03   9.91000000e+02   8.75000000e+02]\n",
      " [  1.05300000e+03   2.59900000e+03   2.59900000e+03]\n",
      " [  4.59000000e+02   2.24000000e+02   2.48800000e+03]\n",
      " [  2.13300000e+03   1.65100000e+03   2.76900000e+03]\n",
      " [  1.00000000e+00   2.35800000e+03   1.09000000e+03]\n",
      " [  2.65900000e+03   2.29900000e+03   1.34400000e+03]\n",
      " [  9.85000000e+02   2.27700000e+03   2.44200000e+03]\n",
      " [  3.30000000e+01   2.22400000e+03   2.31700000e+03]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[ 1886.  1159.     5.]\n",
      " [ 2124.  1946.   615.]\n",
      " [  991.   875.   565.]\n",
      " [ 2599.  2599.  1264.]\n",
      " [  224.  2488.  2637.]\n",
      " [ 1651.  2769.   435.]\n",
      " [ 2358.  1090.  1695.]\n",
      " [ 2299.  1344.  2640.]\n",
      " [ 2277.  2442.   875.]\n",
      " [ 2224.  2317.   109.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[ 2148.  2329.  1063.]\n",
      " [  782.  1065.   226.]\n",
      " [ 1942.   593.  1129.]\n",
      " [  782.  1053.   889.]\n",
      " [ 1361.  2799.   650.]\n",
      " [ 2871.  1850.  2819.]\n",
      " [  426.   438.    33.]\n",
      " [ 1053.  2432.  2367.]\n",
      " [ 1135.  1720.   299.]\n",
      " [ 1885.   575.  1116.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[  2.32900000e+03   1.06300000e+03   1.06300000e+03]\n",
      " [  1.06500000e+03   2.26000000e+02   1.00000000e+00]\n",
      " [  5.93000000e+02   1.12900000e+03   4.63000000e+02]\n",
      " [  1.05300000e+03   8.89000000e+02   2.65900000e+03]\n",
      " [  2.79900000e+03   6.50000000e+02   1.28400000e+03]\n",
      " [  1.85000000e+03   2.81900000e+03   2.43200000e+03]\n",
      " [  4.38000000e+02   3.30000000e+01   9.46000000e+02]\n",
      " [  2.43200000e+03   2.36700000e+03   2.65900000e+03]\n",
      " [  1.72000000e+03   2.99000000e+02   1.28000000e+03]\n",
      " [  5.75000000e+02   1.11600000e+03   1.06400000e+03]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[  828.  1864.  1243.]\n",
      " [ 1516.   249.   593.]\n",
      " [  765.   126.  2799.]\n",
      " [ 1053.  2127.  2224.]\n",
      " [ 1344.  2640.  2329.]\n",
      " [ 2459.  2432.  1098.]\n",
      " [ 1181.   593.   828.]\n",
      " [  981.   264.   682.]\n",
      " [ 1565.  1976.   438.]\n",
      " [ 2413.   430.   396.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[ 1864.  1243.  1577.]\n",
      " [  249.   593.  1665.]\n",
      " [  126.  2799.  1098.]\n",
      " [ 2127.  2224.   575.]\n",
      " [ 2640.  2329.  1457.]\n",
      " [ 2432.  1098.  2722.]\n",
      " [  593.   828.  2405.]\n",
      " [  264.   682.  2147.]\n",
      " [ 1976.   438.   188.]\n",
      " [  430.   396.   438.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[   96.   980.  2262.]\n",
      " [ 2799.  1098.   167.]\n",
      " [ 1528.   249.   593.]\n",
      " [ 2405.   956.   980.]\n",
      " [ 1808.  1361.   287.]\n",
      " [ 2329.  1026.  1135.]\n",
      " [ 2488.  2552.     5.]\n",
      " [ 1053.   828.  1972.]\n",
      " [  282.   593.  2224.]\n",
      " [ 1779.   254.   828.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[  980.  2262.   875.]\n",
      " [ 1098.   167.   611.]\n",
      " [  249.   593.  1184.]\n",
      " [  956.   980.  2871.]\n",
      " [ 1361.   287.  1669.]\n",
      " [ 1026.  1135.  1129.]\n",
      " [ 2552.     5.  1355.]\n",
      " [  828.  1972.  2734.]\n",
      " [  593.  2224.   828.]\n",
      " [  254.   828.   438.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[ 2192.   723.   265.]\n",
      " [ 2413.   438.  1441.]\n",
      " [ 1277.  2659.   575.]\n",
      " [  968.  1090.   812.]\n",
      " [  875.   593.  1135.]\n",
      " [  299.   751.  2507.]\n",
      " [  593.  1985.  1886.]\n",
      " [  739.  2635.  1275.]\n",
      " [ 1224.   242.  1653.]\n",
      " [ 1453.  2765.  1885.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[  723.   265.  2634.]\n",
      " [  438.  1441.  2339.]\n",
      " [ 2659.   575.   858.]\n",
      " [ 1090.   812.  1065.]\n",
      " [  593.  1135.  1863.]\n",
      " [  751.  2507.   643.]\n",
      " [ 1985.  1886.   858.]\n",
      " [ 2635.  1275.   462.]\n",
      " [  242.  1653.  1098.]\n",
      " [ 2765.  1885.   575.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[  1.81400000e+03   2.99000000e+02   1.95100000e+03]\n",
      " [  1.19900000e+03   1.13100000e+03   5.93000000e+02]\n",
      " [  5.93000000e+02   1.05300000e+03   5.90000000e+02]\n",
      " [  1.39200000e+03   4.38000000e+02   1.23000000e+03]\n",
      " [  4.13000000e+02   8.75000000e+02   2.55200000e+03]\n",
      " [  7.82000000e+02   1.05300000e+03   2.64000000e+03]\n",
      " [  2.48000000e+02   2.99000000e+02   1.15000000e+02]\n",
      " [  9.60000000e+02   5.75000000e+02   5.10000000e+02]\n",
      " [  2.14700000e+03   7.82000000e+02   8.30000000e+01]\n",
      " [  2.26000000e+02   1.00000000e+00   1.22300000e+03]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[  2.99000000e+02   1.95100000e+03   2.24500000e+03]\n",
      " [  1.13100000e+03   5.93000000e+02   5.75000000e+02]\n",
      " [  1.05300000e+03   5.90000000e+02   2.14700000e+03]\n",
      " [  4.38000000e+02   1.23000000e+03   8.62000000e+02]\n",
      " [  8.75000000e+02   2.55200000e+03   1.12800000e+03]\n",
      " [  1.05300000e+03   2.64000000e+03   2.32900000e+03]\n",
      " [  2.99000000e+02   1.15000000e+02   1.99600000e+03]\n",
      " [  5.75000000e+02   5.10000000e+02   2.55900000e+03]\n",
      " [  7.82000000e+02   8.30000000e+01   2.41000000e+02]\n",
      " [  1.00000000e+00   1.22300000e+03   8.75000000e+02]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[ 1344.  2640.  2329.]\n",
      " [ 2299.   782.  1065.]\n",
      " [ 2489.    11.  1719.]\n",
      " [  980.  1131.  2432.]\n",
      " [ 2179.  2489.  1847.]\n",
      " [ 2871.  2299.   782.]\n",
      " [ 2025.  1925.   413.]\n",
      " [  812.   593.  2070.]\n",
      " [ 2394.  1932.   593.]\n",
      " [ 2442.   805.   239.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[ 2640.  2329.  2432.]\n",
      " [  782.  1065.   226.]\n",
      " [   11.  1719.  2147.]\n",
      " [ 1131.  2432.  1991.]\n",
      " [ 2489.  1847.  2124.]\n",
      " [ 2299.   782.  1053.]\n",
      " [ 1925.   413.  2010.]\n",
      " [  593.  2070.   396.]\n",
      " [ 1932.   593.  1912.]\n",
      " [  805.   239.   593.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[ 2806.   765.  2215.]\n",
      " [ 1444.   782.  2799.]\n",
      " [ 1444.   782.   204.]\n",
      " [  960.  1995.  2482.]\n",
      " [  593.  1053.  1991.]\n",
      " [ 1614.   782.   940.]\n",
      " [ 2239.   396.  1184.]\n",
      " [  886.   396.   438.]\n",
      " [  782.    16.   782.]\n",
      " [ 2432.   683.  1139.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[  765.  2215.  2432.]\n",
      " [  782.  2799.   538.]\n",
      " [  782.   204.   935.]\n",
      " [ 1995.  2482.  2799.]\n",
      " [ 1053.  1991.  1129.]\n",
      " [  782.   940.   782.]\n",
      " [  396.  1184.  1623.]\n",
      " [  396.   438.   340.]\n",
      " [   16.   782.  2429.]\n",
      " [  683.  1139.  1995.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n",
      "\n",
      "[[ 1135.   510.  2432.]\n",
      " [  529.   593.  2262.]\n",
      " [ 2640.  2329.  1051.]\n",
      " [  191.  2108.  2432.]\n",
      " [  340.   593.  2058.]\n",
      " [  593.  2559.  1184.]\n",
      " [  875.  2659.   782.]\n",
      " [ 1132.  1090.   593.]\n",
      " [ 1602.  2147.   782.]\n",
      " [  438.  2682.    19.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "\n",
      "[[  510.  2432.  1109.]\n",
      " [  593.  2262.  1129.]\n",
      " [ 2329.  1051.  1157.]\n",
      " [ 2108.  2432.  2208.]\n",
      " [  593.  2058.   575.]\n",
      " [ 2559.  1184.  1065.]\n",
      " [ 2659.   782.  1053.]\n",
      " [ 1090.   593.  2248.]\n",
      " [ 2147.   782.  2413.]\n",
      " [ 2682.    19.  2147.]]\n",
      "<NDArray 10x3 @cpu(0)>\n",
      "<class 'mxnet.ndarray.ndarray.NDArray'>\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for item in data_iter:\n",
    "    if count>10:\n",
    "        break\n",
    "    print(item[0])\n",
    "    print(item[1])\n",
    "    print(type(item[1]))\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_iter_consective(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #连续采样\n",
    "    num_patch = len(corpus)//num_steps\n",
    "    num_batch = num_patch//batch_size\n",
    "    \n",
    "    \n",
    "    for i_batch in range(num_batch):\n",
    "        total_data = []\n",
    "        total_label = []\n",
    "        for i_batch_size in range(batch_size):\n",
    "            start_pos = i_batch+num_batch*(i_batch_size)\n",
    "            data = corpus[start_pos*num_steps:(start_pos+1)*num_steps]\n",
    "            #判断最后是否越界\n",
    "            if(start_pos+1)*num_steps < len(corpus):\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps]\n",
    "                label.append('0')\n",
    "            total_data.append(data)\n",
    "            total_label.append(label)\n",
    "        total_data = mx.ndarray.array(total_data)\n",
    "        total_label = mx.ndarray.array(total_label)\n",
    "        yield(total_data,total_label)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#验证 iter是否正确\n",
    "def valid_data_iter():\n",
    "    corpus = list(range(10000))\n",
    "    batch_size = 10\n",
    "    num_steps = 3\n",
    "    data_iter_rand = data_iter_consective(batch_size=batch_size,num_steps=num_steps,corpus=corpus)\n",
    "    count = 0\n",
    "    for item in data_iter_rand:\n",
    "        if(count>10):\n",
    "            break;\n",
    "        count +=1\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据还需要编码，采用最简单的one-hot-encoding 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "for data,label in data_iter_consective(3,10,idx_corpus):\n",
    "    inp =[nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    break\n",
    "    \n",
    "def one_hot(data,vocab_size):\n",
    "    inp = [nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    #len(inp) = num_steps\n",
    "    return inp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construct rnn\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "input_dim = vocab_size\n",
    "batch_size  = 10\n",
    "std = .01\n",
    "def get_params():\n",
    "    #set params\n",
    "    W_xh= nd.random_normal(scale=std,shape=(input_dim,hidden_dim))\n",
    "    W_hh = nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim))\n",
    "    b_h = nd.zeros(hidden_dim)\n",
    "    \n",
    "    W_hy = nd.random_normal(scale = std,shape=(hidden_dim,output_dim))\n",
    "    b_y = nd.zeros(output_dim)\n",
    "    params = [W_xh,W_hh,b_h,W_hy,b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rnn(inputs,states,*params):\n",
    "    #inputs  is a list shape = batch_size * input_dim\n",
    "    H = states\n",
    "    W_xh,W_hh,b_h,W_hy,b_y = params\n",
    "    outputs = [] \n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)\n",
    "        Y = nd.tanh(nd.dot(H,W_hy)+b_y)\n",
    "        outputs.append(Y)\n",
    "    return (outputs,H)\n",
    "        \n",
    "#test\n",
    "params = get_params()\n",
    "states = nd.zeros((batch_size,hidden_dim))\n",
    "inputs = [nd.ones((batch_size,input_dim))] *3\n",
    "(outputs,H) = rnn(inputs,states,*params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training and inference\n",
    "\n",
    "def predict_rnn(rnn,prefix,vocab_size,num_char,params,hidden_dim,char_to_idx,idx_to_char,ctx=mx.cpu()):\n",
    "    H = nd.zeros(shape=(1,hidden_dim),ctx=ctx)\n",
    "    outputs = []\n",
    "    print(char_to_idx[prefix[0]])\n",
    "    outputs.append(char_to_idx[prefix[0]])\n",
    "    \n",
    "    for i in range(num_char+len(prefix)):\n",
    "        x = nd.array([outputs[-1]])\n",
    "        x = one_hot(x,vocab_size)\n",
    "        out, H = rnn(x,H,*params)\n",
    "        if i< len(prefix)-1:\n",
    "            new_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            new_input  = int(out[-1].argmax(axis=1).asscalar())\n",
    "        outputs.append(new_input)\n",
    "    print(outputs)\n",
    "    return ''.join([idx_to_char[x] for x in outputs])\n",
    "    \n",
    "#prefix='书由 nainia'\n",
    "#predict_rnn(rnn,prefix,vocab_size,10,params,hidden_dim,char_to_idx,idx_to_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient cliping\n",
    "def grad_clipping(params,theta,ctx):\n",
    "    #python传参是传的引用，所有不用返回值\n",
    "    if theta is not None:\n",
    "        norm = nd.array([0.0],ctx=ctx)\n",
    "        for p in params:\n",
    "            norm += nd.sum(p.grad**2)\n",
    "        norm = nd.sqrt(norm).asscalar()\n",
    "        if norm > theta:\n",
    "            for p in params:\n",
    "                p.grad[:] *= theta/norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "#perplexity 困惑度\n",
    "def train_and_predict(num_steps,ctx=mx.cpu()):\n",
    "    #how?\n",
    "    #定义损失\n",
    "    loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    #生成数据迭代器\n",
    "    data_iter = data_iter_random(num_steps,batch_size,idx_corpus)\n",
    "    #循环num_epoch\n",
    "    num_epochs = 1\n",
    "    \n",
    "    params = get_params()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = nd.array([0.0])\n",
    "        H = nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx)\n",
    "        for data,label in data_iter:\n",
    "            data = one_hot(nd.array(data),vocab_size)\n",
    "            label = nd.array(label)\n",
    "            with mx.autograd.record():\n",
    "                out,H = rnn(data,H,*params)\n",
    "                #计算loss\n",
    "                for i in range(len(out)):\n",
    "                    train_loss += nd.mean(loss(out[i],label[:,i])).asscalar()\n",
    "                    #loss(out[i],label[:,i])\n",
    "        print(train_loss)\n",
    "            #train_loss.backward()\n",
    "            #mean_loss = nd.mean(train_loss).asscalar()\n",
    "            #print('loss:%f'%mean_loss)\n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps=3\n",
    "train_and_predict(num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
