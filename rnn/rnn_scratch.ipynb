{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64925\n"
     ]
    }
   ],
   "source": [
    "with open('./input/jaychou_lyrics.txt','r',encoding='utf-8') as fh:\n",
    "    lines = fh.read()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = lines.replace('\\n', ' ').replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每天在想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的让我心疼的可'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct data iterator \n",
    "content = content[0:20000]\n",
    "idx_to_char = list(set(content))\n",
    "char_to_idx = dict([(char,int(i)) for i,char in enumerate(idx_to_char)])\n",
    "vocab_size = len(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1465"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_corpus = [char_to_idx[c] for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n"
     ]
    }
   ],
   "source": [
    "s = ''.join([idx_to_char[i] for i in idx_corpus])\n",
    "print(s[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import update\n",
    "\n",
    "def data_iter_random(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #随机生成 data iterator，用yield返回？？\n",
    "    #先随机分段\n",
    "    #再随机选择batch_size个字段\n",
    "    len_corpus=len(corpus)\n",
    "    num_patch = len_corpus//num_steps\n",
    "    num_batch = num_patch//batch_size \n",
    "    idx_list = [x for x in range(num_patch)]\n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    for batch in range(num_batch):\n",
    "        #在idx_list中随机选择batch_size 个元素，选完之后在idx_list中删除这些元素\n",
    "        to_use = random.sample(idx_list,batch_size)\n",
    "        batch_data = []\n",
    "        batch_label = []\n",
    "        for item in to_use:\n",
    "            idx_list.remove(item)\n",
    "        for step in to_use:\n",
    "            #i * num_patch ~ (i+1)*num_path\n",
    "            data = corpus[step*num_steps:(step+1)*num_steps]\n",
    "            batch_data.append(data)\n",
    "            #判断最后一个是否越界\n",
    "            if (step+1)*num_steps < len_corpus:\n",
    "                label = corpus[step*num_steps + 1:(step+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[step*num_steps+1:(step+1)*num_steps]\n",
    "                label.append('0')#填充字符\n",
    "            batch_label.append(label)\n",
    "        #聚合\n",
    "        #ndarray batch_size * num_steps\n",
    "        batch_data = mx.ndarray.array(batch_data)\n",
    "        batch_label = mx.ndarray.array(batch_label)\n",
    "        yield(batch_data,batch_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data_iter = data_iter_random(3,10,idx_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count = 1\n",
    "#for item in data_iter:\n",
    "#    if count>10:\n",
    "#        break\n",
    "#    print(item[0])\n",
    "#    print(item[1])\n",
    "#    print(type(item[1]))\n",
    "#    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_iter_consective(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #连续采样\n",
    "    num_patch = len(corpus)//num_steps\n",
    "    num_batch = num_patch//batch_size\n",
    "    \n",
    "    \n",
    "    for i_batch in range(num_batch):\n",
    "        total_data = []\n",
    "        total_label = []\n",
    "        for i_batch_size in range(batch_size):\n",
    "            start_pos = i_batch+num_batch*(i_batch_size)\n",
    "            data = corpus[start_pos*num_steps:(start_pos+1)*num_steps]\n",
    "            #判断最后是否越界\n",
    "            if(start_pos+1)*num_steps < len(corpus):\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps]\n",
    "                label.append('0')\n",
    "            total_data.append(data)\n",
    "            total_label.append(label)\n",
    "        total_data = mx.ndarray.array(total_data)\n",
    "        total_label = mx.ndarray.array(total_label)\n",
    "        yield(total_data,total_label)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#验证 iter是否正确\n",
    "def valid_data_iter():\n",
    "    corpus = list(range(10000))\n",
    "    batch_size = 10\n",
    "    num_steps = 3\n",
    "    data_iter_rand = data_iter_consective(batch_size=batch_size,num_steps=num_steps,corpus=corpus)\n",
    "    count = 0\n",
    "    for item in data_iter_rand:\n",
    "        if(count>10):\n",
    "            break;\n",
    "        count +=1\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据还需要编码，采用最简单的one-hot-encoding 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "for data,label in data_iter_consective(3,10,idx_corpus):\n",
    "    inp =[nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    break\n",
    "    \n",
    "def one_hot(data,vocab_size):\n",
    "    inp = [nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    #len(inp) = num_steps\n",
    "    return inp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#尝试使用gpu\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import util\n",
    "ctx=util.try_gpu()\n",
    "\n",
    "#construct rnn\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "input_dim = vocab_size\n",
    "batch_size  = 32\n",
    "std = .01\n",
    "learning_rate = 0.1\n",
    "def get_params():\n",
    "    #set params\n",
    "    W_xh= nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx)\n",
    "    W_hh = nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx)\n",
    "    b_h = nd.zeros(hidden_dim,ctx=ctx)\n",
    "    \n",
    "    W_hy = nd.random_normal(scale = std,shape=(hidden_dim,output_dim),ctx=ctx)\n",
    "    b_y = nd.zeros(output_dim,ctx=ctx)\n",
    "    params = [W_xh,W_hh,b_h,W_hy,b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output len:3\n",
      "output[0] shape (32, 1465)\n"
     ]
    }
   ],
   "source": [
    "def rnn(inputs,states,*params):\n",
    "    #inputs  is a list shape = batch_size * input_dim\n",
    "    H = states\n",
    "    W_xh,W_hh,b_h,W_hy,b_y = params\n",
    "    outputs = [] \n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)\n",
    "        Y = nd.dot(H,W_hy)+b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs,H)\n",
    "        \n",
    "#test\n",
    "params = get_params()\n",
    "states = nd.zeros((batch_size,hidden_dim),ctx=ctx)\n",
    "inputs = [nd.ones((batch_size,input_dim),ctx=ctx)] *3\n",
    "(outputs,H) = rnn(inputs,states,*params)\n",
    "print('output len:%d'%len(outputs))\n",
    "print('output[0] shape',outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training and inference\n",
    "\n",
    "def predict_rnn(rnn,prefix,vocab_size,num_char,params,hidden_dim,char_to_idx,idx_to_char):\n",
    "    H = nd.zeros(shape=(1,hidden_dim),ctx=ctx)\n",
    "    outputs = []\n",
    "    print(char_to_idx[prefix[0]])\n",
    "    outputs.append(char_to_idx[prefix[0]])\n",
    "    \n",
    "    for i in range(num_char+len(prefix)):\n",
    "        x = nd.array([outputs[-1]],ctx=ctx)\n",
    "        x = one_hot(x,vocab_size)\n",
    "        out, H = rnn(x,H,*params)\n",
    "        if i< len(prefix)-1:\n",
    "            new_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            new_input  = int(out[-1].argmax(axis=1).asscalar())\n",
    "        outputs.append(new_input)\n",
    "    print(outputs)\n",
    "    return ''.join([idx_to_char[x] for x in outputs])\n",
    "    \n",
    "#prefix='书由 nainia'\n",
    "#predict_rnn(rnn,prefix,vocab_size,10,params,hidden_dim,char_to_idx,idx_to_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient cliping\n",
    "def grad_clipping(params,theta):\n",
    "    #python传参是传的引用，所有不用返回值\n",
    "    norm = nd.array([0.0],ctx=ctx)\n",
    "    for p in params:\n",
    "        norm = norm + nd.sum(p.grad**2)\n",
    "    norm = nd.sqrt(norm).asscalar()\n",
    "    if norm > theta:\n",
    "        for p in params:\n",
    "            p.grad[:] =p.grad[:]* theta/norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "#perplexity 困惑度\n",
    "import numpy as np\n",
    "def train_and_predict(num_steps):\n",
    "    #how?\n",
    "    #定义损失\n",
    "    loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    #生成数据迭代器\n",
    "    #循环num_epoch\n",
    "    num_epochs = 10\n",
    "    \n",
    "    params = get_params()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        data_iter = data_iter_random(num_steps,batch_size,idx_corpus)\n",
    "        num_example = 0.0\n",
    "        for data,label in data_iter:\n",
    "            data = one_hot(nd.array(data,ctx=ctx),vocab_size)\n",
    "            H = nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx)\n",
    "            label = nd.array(label,ctx=ctx)\n",
    "            with mx.autograd.record():\n",
    "                out,H = rnn(data,H,*params)\n",
    "                #计算loss\n",
    "                #for i in range(len(out)):\n",
    "                #    myloss = loss(out[i],label[:,i])\n",
    "                #    train_loss += nd.sum(myloss).asscalar()\n",
    "                label = label.T.reshape((-1,))\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                myloss=loss(outputs,label)\n",
    "            myloss.backward()\n",
    "            grad_clipping(params,5)\n",
    "            update(params,learning_rate)\n",
    "            train_loss += nd.sum(myloss).asscalar()\n",
    "            num_example += myloss.size\n",
    "        print(np.exp(train_loss/num_example))\n",
    "        print(num_example)\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'outputs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d07c18357cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m35\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_and_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-f2f835959c04>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m(num_steps)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[1;31m#    train_loss += nd.sum(myloss).asscalar()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 \u001b[0mmyloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mmyloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'outputs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "num_steps=35\n",
    "train_and_predict(num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
