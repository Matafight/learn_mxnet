{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64925\n"
     ]
    }
   ],
   "source": [
    "with open('./input/jaychou_lyrics.txt','r',encoding='utf-8') as fh:\n",
    "    lines = fh.read()\n",
    "    print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content = lines.replace('\\n', ' ').replace('\\r', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每天在想想想想著你 这样的甜蜜 让我开始乡相信命运 感谢地心引力 让我碰到你 漂亮的让我面红的可爱女人 温柔的让我心疼的可'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct data iterator \n",
    "content = content[0:20000]\n",
    "idx_to_char = list(set(content))\n",
    "char_to_idx = dict([(char,int(i)) for i,char in enumerate(idx_to_char)])\n",
    "vocab_size = len(idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1465"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_corpus = [char_to_idx[c] for c in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "想要有直升机 想要和你飞到宇宙去 想要和你融化在一起 融化在宇宙里 我每天每天每\n"
     ]
    }
   ],
   "source": [
    "s = ''.join([idx_to_char[i] for i in idx_corpus])\n",
    "print(s[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import mxnet as mx\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import update\n",
    "\n",
    "def data_iter_random(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #随机生成 data iterator，用yield返回？？\n",
    "    #先随机分段\n",
    "    #再随机选择batch_size个字段\n",
    "    len_corpus=len(corpus)\n",
    "    num_patch = len_corpus//num_steps\n",
    "    num_batch = num_patch//batch_size \n",
    "    idx_list = [x for x in range(num_patch)]\n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    for batch in range(num_batch):\n",
    "        #在idx_list中随机选择batch_size 个元素，选完之后在idx_list中删除这些元素\n",
    "        to_use = random.sample(idx_list,batch_size)\n",
    "        batch_data = []\n",
    "        batch_label = []\n",
    "        for item in to_use:\n",
    "            idx_list.remove(item)\n",
    "        for step in to_use:\n",
    "            #i * num_patch ~ (i+1)*num_path\n",
    "            data = corpus[step*num_steps:(step+1)*num_steps]\n",
    "            batch_data.append(data)\n",
    "            #判断最后一个是否越界\n",
    "            if (step+1)*num_steps < len_corpus:\n",
    "                label = corpus[step*num_steps + 1:(step+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[step*num_steps+1:(step+1)*num_steps]\n",
    "                label.append('0')#填充字符\n",
    "            batch_label.append(label)\n",
    "        #聚合\n",
    "        #ndarray batch_size * num_steps\n",
    "        batch_data = mx.ndarray.array(batch_data)\n",
    "        batch_label = mx.ndarray.array(batch_label)\n",
    "        yield(batch_data,batch_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "data_iter = data_iter_random(3,10,idx_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_iter_consective(num_steps,batch_size,corpus,ctx=mx.cpu()):\n",
    "    #连续采样\n",
    "    num_patch = len(corpus)//num_steps\n",
    "    num_batch = num_patch//batch_size\n",
    "    \n",
    "    \n",
    "    for i_batch in range(num_batch):\n",
    "        total_data = []\n",
    "        total_label = []\n",
    "        for i_batch_size in range(batch_size):\n",
    "            start_pos = i_batch+num_batch*(i_batch_size)\n",
    "            data = corpus[start_pos*num_steps:(start_pos+1)*num_steps]\n",
    "            #判断最后是否越界\n",
    "            if(start_pos+1)*num_steps < len(corpus):\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps+1]\n",
    "            else:\n",
    "                label = corpus[start_pos*num_steps+1:(start_pos+1)*num_steps]\n",
    "                label.append('0')\n",
    "            total_data.append(data)\n",
    "            total_label.append(label)\n",
    "        total_data = mx.ndarray.array(total_data)\n",
    "        total_label = mx.ndarray.array(total_label)\n",
    "        yield(total_data,total_label)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#验证 iter是否正确\n",
    "def valid_data_iter():\n",
    "    corpus = list(range(10000))\n",
    "    batch_size = 10\n",
    "    num_steps = 3\n",
    "    data_iter_rand = data_iter_consective(batch_size=batch_size,num_steps=num_steps,corpus=corpus)\n",
    "    count = 0\n",
    "    for item in data_iter_rand:\n",
    "        if(count>10):\n",
    "            break;\n",
    "        count +=1\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据还需要编码，采用最简单的one-hot-encoding 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd\n",
    "for data,label in data_iter_consective(3,10,idx_corpus):\n",
    "    inp =[nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    break\n",
    "    \n",
    "def one_hot(data,vocab_size):\n",
    "    inp = [nd.one_hot(x,vocab_size) for x in data.T]\n",
    "    #len(inp) = num_steps\n",
    "    return inp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#尝试使用gpu\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import util\n",
    "ctx=util.try_gpu()\n",
    "\n",
    "#construct rnn\n",
    "hidden_dim = 256\n",
    "output_dim = vocab_size\n",
    "input_dim = vocab_size\n",
    "batch_size  = 32\n",
    "std = .01\n",
    "learning_rate = 0.2\n",
    "def get_params():\n",
    "    #set params\n",
    "    W_xh= nd.random_normal(scale=std,shape=(input_dim,hidden_dim),ctx=ctx)\n",
    "    W_hh = nd.random_normal(scale=std,shape=(hidden_dim,hidden_dim),ctx=ctx)\n",
    "    b_h = nd.zeros(hidden_dim,ctx=ctx)\n",
    "    \n",
    "    W_hy = nd.random_normal(scale = std,shape=(hidden_dim,output_dim),ctx=ctx)\n",
    "    b_y = nd.zeros(output_dim,ctx=ctx)\n",
    "    params = [W_xh,W_hh,b_h,W_hy,b_y]\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output len:3\n",
      "output[0] shape (32, 1465)\n"
     ]
    }
   ],
   "source": [
    "def rnn(inputs,states,*params):\n",
    "    #inputs  is a list shape = batch_size * input_dim\n",
    "    H = states\n",
    "    W_xh,W_hh,b_h,W_hy,b_y = params\n",
    "    outputs = [] \n",
    "    for X in inputs:\n",
    "        H = nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)\n",
    "        Y = nd.dot(H,W_hy)+b_y\n",
    "        outputs.append(Y)\n",
    "    return (outputs,H)\n",
    "        \n",
    "#test\n",
    "params = get_params()\n",
    "states = nd.zeros((batch_size,hidden_dim),ctx=ctx)\n",
    "inputs = [nd.ones((batch_size,input_dim),ctx=ctx)] *3\n",
    "(outputs,H) = rnn(inputs,states,*params)\n",
    "print('output len:%d'%len(outputs))\n",
    "print('output[0] shape',outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training and inference\n",
    "\n",
    "def predict_rnn(rnn,prefix,vocab_size,num_char,params,hidden_dim,char_to_idx,idx_to_char):\n",
    "    H = nd.zeros(shape=(1,hidden_dim),ctx=ctx)\n",
    "    outputs = []\n",
    "    print(char_to_idx[prefix[0]])\n",
    "    outputs.append(char_to_idx[prefix[0]])\n",
    "    \n",
    "    for i in range(num_char+len(prefix)):\n",
    "        x = nd.array([outputs[-1]],ctx=ctx)\n",
    "        x = one_hot(x,vocab_size)\n",
    "        out, H = rnn(x,H,*params)\n",
    "        if i< len(prefix)-1:\n",
    "            new_input = char_to_idx[prefix[i+1]]\n",
    "        else:\n",
    "            new_input  = int(out[-1].argmax(axis=1).asscalar())\n",
    "        outputs.append(new_input)\n",
    "    print(outputs)\n",
    "    return ''.join([idx_to_char[x] for x in outputs])\n",
    "    \n",
    "#prefix='书由 nainia'\n",
    "#predict_rnn(rnn,prefix,vocab_size,10,params,hidden_dim,char_to_idx,idx_to_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradient cliping\n",
    "def grad_clipping(params,theta):\n",
    "    #python传参是传的引用，所有不用返回值\n",
    "    norm = nd.array([0.0],ctx=ctx)\n",
    "    for p in params:\n",
    "        norm = norm + nd.sum(p.grad**2)\n",
    "    norm = nd.sqrt(norm).asscalar()\n",
    "    if norm > theta:\n",
    "        for p in params:\n",
    "            p.grad[:] =p.grad[:]* theta/norm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# training\n",
    "#perplexity 困惑度\n",
    "import numpy as np\n",
    "def train_and_predict(num_steps):\n",
    "    #how?\n",
    "    #定义损失\n",
    "    loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    \n",
    "    #生成数据迭代器\n",
    "    #循环num_epoch\n",
    "    num_epochs = 200\n",
    "    \n",
    "    params = get_params()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        data_iter = data_iter_random(num_steps,batch_size,idx_corpus)\n",
    "        num_example = 0.0\n",
    "        for data,label in data_iter:\n",
    "            data = one_hot(nd.array(data,ctx=ctx),vocab_size)\n",
    "            H = nd.zeros(shape=(batch_size,hidden_dim),ctx=ctx)\n",
    "            label = nd.array(label,ctx=ctx)\n",
    "            with mx.autograd.record():\n",
    "                outputs,H = rnn(data,H,*params)\n",
    "                #计算loss\n",
    "                #for i in range(len(out)):\n",
    "                #    myloss = loss(out[i],label[:,i])\n",
    "                #    train_loss += nd.sum(myloss).asscalar()\n",
    "                label = label.T.reshape((-1,))\n",
    "                outputs = nd.concat(*outputs, dim=0)\n",
    "                myloss=loss(outputs,label)\n",
    "            myloss.backward()\n",
    "            grad_clipping(params,5)\n",
    "            update(params,learning_rate)\n",
    "            train_loss += nd.sum(myloss).asscalar()\n",
    "            num_example += myloss.size\n",
    "        print('Epoch:%d  Perplexity:%f'%(epoch,np.exp(train_loss/num_example)))\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0  Perplexity:959.586310\n",
      "Epoch:1  Perplexity:546.509495\n",
      "Epoch:2  Perplexity:437.885582\n",
      "Epoch:3  Perplexity:414.579982\n",
      "Epoch:4  Perplexity:393.466083\n",
      "Epoch:5  Perplexity:388.807105\n",
      "Epoch:6  Perplexity:369.666898\n",
      "Epoch:7  Perplexity:361.338033\n",
      "Epoch:8  Perplexity:346.840948\n",
      "Epoch:9  Perplexity:330.855805\n",
      "Epoch:10  Perplexity:321.061496\n",
      "Epoch:11  Perplexity:308.937507\n",
      "Epoch:12  Perplexity:297.752650\n",
      "Epoch:13  Perplexity:281.533243\n",
      "Epoch:14  Perplexity:264.842555\n",
      "Epoch:15  Perplexity:258.996952\n",
      "Epoch:16  Perplexity:244.604761\n",
      "Epoch:17  Perplexity:236.925651\n",
      "Epoch:18  Perplexity:225.914487\n",
      "Epoch:19  Perplexity:217.349734\n",
      "Epoch:20  Perplexity:210.800761\n",
      "Epoch:21  Perplexity:197.042934\n",
      "Epoch:22  Perplexity:192.416102\n",
      "Epoch:23  Perplexity:181.631587\n",
      "Epoch:24  Perplexity:173.422607\n",
      "Epoch:25  Perplexity:163.424871\n",
      "Epoch:26  Perplexity:158.291233\n",
      "Epoch:27  Perplexity:150.555297\n",
      "Epoch:28  Perplexity:143.639752\n",
      "Epoch:29  Perplexity:135.710125\n",
      "Epoch:30  Perplexity:128.804408\n",
      "Epoch:31  Perplexity:122.319104\n",
      "Epoch:32  Perplexity:119.182146\n",
      "Epoch:33  Perplexity:112.054504\n",
      "Epoch:34  Perplexity:108.253279\n",
      "Epoch:35  Perplexity:102.285964\n",
      "Epoch:36  Perplexity:97.136691\n",
      "Epoch:37  Perplexity:92.359573\n",
      "Epoch:38  Perplexity:89.231491\n",
      "Epoch:39  Perplexity:84.897537\n",
      "Epoch:40  Perplexity:78.719277\n",
      "Epoch:41  Perplexity:75.627486\n",
      "Epoch:42  Perplexity:72.283245\n",
      "Epoch:43  Perplexity:67.812863\n",
      "Epoch:44  Perplexity:65.372757\n",
      "Epoch:45  Perplexity:61.770304\n",
      "Epoch:46  Perplexity:59.178663\n",
      "Epoch:47  Perplexity:56.364768\n",
      "Epoch:48  Perplexity:52.560282\n",
      "Epoch:49  Perplexity:50.962169\n",
      "Epoch:50  Perplexity:47.669504\n",
      "Epoch:51  Perplexity:45.772899\n",
      "Epoch:52  Perplexity:43.828571\n",
      "Epoch:53  Perplexity:40.654480\n",
      "Epoch:54  Perplexity:39.495075\n",
      "Epoch:55  Perplexity:37.238658\n",
      "Epoch:56  Perplexity:35.263383\n",
      "Epoch:57  Perplexity:33.449466\n",
      "Epoch:58  Perplexity:32.371473\n",
      "Epoch:59  Perplexity:30.377595\n",
      "Epoch:60  Perplexity:29.168050\n",
      "Epoch:61  Perplexity:27.861908\n",
      "Epoch:62  Perplexity:26.893130\n",
      "Epoch:63  Perplexity:25.705620\n",
      "Epoch:64  Perplexity:24.427838\n",
      "Epoch:65  Perplexity:23.551257\n",
      "Epoch:66  Perplexity:22.820990\n",
      "Epoch:67  Perplexity:21.402724\n",
      "Epoch:68  Perplexity:20.409422\n",
      "Epoch:69  Perplexity:19.984963\n",
      "Epoch:70  Perplexity:18.821510\n",
      "Epoch:71  Perplexity:18.256474\n",
      "Epoch:72  Perplexity:17.835858\n",
      "Epoch:73  Perplexity:16.868765\n",
      "Epoch:74  Perplexity:16.040645\n",
      "Epoch:75  Perplexity:15.743613\n",
      "Epoch:76  Perplexity:15.367280\n",
      "Epoch:77  Perplexity:14.654142\n",
      "Epoch:78  Perplexity:13.990879\n",
      "Epoch:79  Perplexity:13.229038\n",
      "Epoch:80  Perplexity:12.962035\n",
      "Epoch:81  Perplexity:12.542115\n",
      "Epoch:82  Perplexity:12.352701\n",
      "Epoch:83  Perplexity:11.727669\n",
      "Epoch:84  Perplexity:11.537519\n",
      "Epoch:85  Perplexity:10.833853\n",
      "Epoch:86  Perplexity:10.587062\n",
      "Epoch:87  Perplexity:10.134635\n",
      "Epoch:88  Perplexity:9.888551\n",
      "Epoch:89  Perplexity:9.848129\n",
      "Epoch:90  Perplexity:9.361679\n",
      "Epoch:91  Perplexity:9.208894\n",
      "Epoch:92  Perplexity:8.748896\n",
      "Epoch:93  Perplexity:8.665058\n",
      "Epoch:94  Perplexity:8.318925\n",
      "Epoch:95  Perplexity:8.231031\n",
      "Epoch:96  Perplexity:7.937660\n",
      "Epoch:97  Perplexity:7.785616\n",
      "Epoch:98  Perplexity:7.456245\n",
      "Epoch:99  Perplexity:7.249544\n",
      "Epoch:100  Perplexity:7.057954\n",
      "Epoch:101  Perplexity:7.070777\n",
      "Epoch:102  Perplexity:6.773365\n",
      "Epoch:103  Perplexity:6.698177\n",
      "Epoch:104  Perplexity:6.462281\n",
      "Epoch:105  Perplexity:6.282019\n",
      "Epoch:106  Perplexity:6.218653\n",
      "Epoch:107  Perplexity:6.107514\n",
      "Epoch:108  Perplexity:5.962276\n",
      "Epoch:109  Perplexity:5.823790\n",
      "Epoch:110  Perplexity:5.666515\n",
      "Epoch:111  Perplexity:5.511625\n",
      "Epoch:112  Perplexity:5.473690\n",
      "Epoch:113  Perplexity:5.319838\n",
      "Epoch:114  Perplexity:5.362241\n",
      "Epoch:115  Perplexity:5.134231\n",
      "Epoch:116  Perplexity:4.961620\n",
      "Epoch:117  Perplexity:5.022345\n",
      "Epoch:118  Perplexity:4.827501\n",
      "Epoch:119  Perplexity:4.820069\n",
      "Epoch:120  Perplexity:4.700767\n",
      "Epoch:121  Perplexity:4.545584\n",
      "Epoch:122  Perplexity:4.627553\n",
      "Epoch:123  Perplexity:4.472644\n",
      "Epoch:124  Perplexity:4.349687\n",
      "Epoch:125  Perplexity:4.283469\n",
      "Epoch:126  Perplexity:4.246001\n",
      "Epoch:127  Perplexity:4.102909\n",
      "Epoch:128  Perplexity:4.092836\n",
      "Epoch:129  Perplexity:4.130329\n",
      "Epoch:130  Perplexity:3.990851\n",
      "Epoch:131  Perplexity:3.916619\n",
      "Epoch:132  Perplexity:3.855785\n",
      "Epoch:133  Perplexity:3.856035\n",
      "Epoch:134  Perplexity:3.812015\n",
      "Epoch:135  Perplexity:3.716373\n",
      "Epoch:136  Perplexity:3.621710\n",
      "Epoch:137  Perplexity:3.662898\n",
      "Epoch:138  Perplexity:3.589376\n",
      "Epoch:139  Perplexity:3.512011\n",
      "Epoch:140  Perplexity:3.532977\n",
      "Epoch:141  Perplexity:3.522618\n",
      "Epoch:142  Perplexity:3.435717\n",
      "Epoch:143  Perplexity:3.355292\n",
      "Epoch:144  Perplexity:3.265850\n",
      "Epoch:145  Perplexity:3.276273\n",
      "Epoch:146  Perplexity:3.307590\n",
      "Epoch:147  Perplexity:3.237165\n",
      "Epoch:148  Perplexity:3.271405\n",
      "Epoch:149  Perplexity:3.159599\n",
      "Epoch:150  Perplexity:3.140501\n",
      "Epoch:151  Perplexity:3.095597\n",
      "Epoch:152  Perplexity:3.041597\n",
      "Epoch:153  Perplexity:3.037365\n",
      "Epoch:154  Perplexity:2.980734\n",
      "Epoch:155  Perplexity:2.991312\n",
      "Epoch:156  Perplexity:2.987029\n",
      "Epoch:157  Perplexity:2.938182\n",
      "Epoch:158  Perplexity:2.932258\n",
      "Epoch:159  Perplexity:2.856330\n",
      "Epoch:160  Perplexity:2.850628\n",
      "Epoch:161  Perplexity:2.781258\n",
      "Epoch:162  Perplexity:2.831997\n",
      "Epoch:163  Perplexity:2.813973\n",
      "Epoch:164  Perplexity:2.778037\n",
      "Epoch:165  Perplexity:2.726215\n",
      "Epoch:166  Perplexity:2.716383\n",
      "Epoch:167  Perplexity:2.723628\n",
      "Epoch:168  Perplexity:2.715313\n",
      "Epoch:169  Perplexity:2.683714\n",
      "Epoch:170  Perplexity:2.630924\n",
      "Epoch:171  Perplexity:2.622367\n",
      "Epoch:172  Perplexity:2.644368\n",
      "Epoch:173  Perplexity:2.532550\n",
      "Epoch:174  Perplexity:2.582987\n",
      "Epoch:175  Perplexity:2.603361\n",
      "Epoch:176  Perplexity:2.546014\n",
      "Epoch:177  Perplexity:2.545099\n",
      "Epoch:178  Perplexity:2.522328\n",
      "Epoch:179  Perplexity:2.534623\n",
      "Epoch:180  Perplexity:2.498476\n",
      "Epoch:181  Perplexity:2.493680\n",
      "Epoch:182  Perplexity:2.435543\n",
      "Epoch:183  Perplexity:2.425131\n",
      "Epoch:184  Perplexity:2.375123\n",
      "Epoch:185  Perplexity:2.432006\n",
      "Epoch:186  Perplexity:2.410209\n",
      "Epoch:187  Perplexity:2.509867\n",
      "Epoch:188  Perplexity:2.451065\n",
      "Epoch:189  Perplexity:2.450033\n",
      "Epoch:190  Perplexity:2.429702\n",
      "Epoch:191  Perplexity:2.411729\n",
      "Epoch:192  Perplexity:2.340772\n",
      "Epoch:193  Perplexity:2.395779\n",
      "Epoch:194  Perplexity:2.326082\n",
      "Epoch:195  Perplexity:2.340672\n",
      "Epoch:196  Perplexity:2.283708\n",
      "Epoch:197  Perplexity:2.305643\n",
      "Epoch:198  Perplexity:2.298409\n",
      "Epoch:199  Perplexity:2.262890\n"
     ]
    }
   ],
   "source": [
    "num_steps=35\n",
    "train_and_predict(num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet.ndarray as nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = nd.ones((10,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]\n",
       " [ 1.  1.  1.]]\n",
       "<NDArray 10x3 @cpu(0)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
       "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
       "<NDArray 30 @cpu(0)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
